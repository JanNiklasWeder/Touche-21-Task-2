{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassMerge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muWnvVoBuYes",
        "outputId": "fdf8cadd-f298-474f-c0dd-6664887f2561"
      },
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "'''\n",
        "input for each topic:\n",
        "\n",
        "+ topic_id\n",
        "+ res:\n",
        "    resp[tag] = chatnoir_resp\n",
        "    resp[sensevec_1] = chatnoir_resp\n",
        "    resp[sensevec_2] = chatnoir_resp\n",
        "    ...\n",
        "+ weights:\n",
        "    weights['original'] = \n",
        "    weights['preprocessing']=\n",
        "    weights['annotation'] = \n",
        "    weights['syns'] = \n",
        "    weights['sensevec'] = (all queries of 'sensevec' haben same weights)\n",
        "    weights['embedded'] = (all queries of 'embedded' haben same weights)\n",
        "\n",
        "+method: max or mean\n",
        "output:\n",
        "'''\n",
        "\n",
        "class Merge:\n",
        "\n",
        "    def __init__(self, topic_id: int, resp: dict, weights: dict, method: str):\n",
        "        self.topic_id = topic_id\n",
        "        self.res_tags = resp\n",
        "        self.weights = weights\n",
        "        self.method = method\n",
        "\n",
        "    def merging_res(self):\n",
        "        #resp['results'] = [doc1, doc2, ...]\n",
        "        updated_resp_tags=[]\n",
        "        for tag, resp in self.res_tags.items():\n",
        "            updated_resp = self.update_scores_by_tags(tag, resp['results']) #(tag, updated_resp)\n",
        "            updated_resp_tags.append(updated_resp)\n",
        "        \n",
        "        sorted_updated_resp_tags = list(chain.from_iterable([updated_resp[1] for updated_resp in updated_resp_tags]))\n",
        "        #sort list by update_score: list of docs \n",
        "        sorted_updated_resp_tags = sorted(sorted_updated_resp_tags, key=lambda doc: doc['updated_score'], reverse=True)\n",
        "        #find docs with same trec-id = trec-id appear multiple times\n",
        "        trec_ids = [doc['trec_id'] for doc in sorted_updated_resp_tags]\n",
        "        multiple_ids = [trec_id for trec_id, count in dict(Counter(trec_ids)).items() if count!=1]\n",
        "        \n",
        "        merged_resp=[]\n",
        "        if multiple_ids!=[]:\n",
        "            merged_resp = self.mergen_for_multiple_ids(sorted_updated_resp_tags, multiple_ids)\n",
        "        else:\n",
        "            merged_resp = sorted_updated_resp_tags\n",
        "        return {'results':merged_resp}\n",
        "\n",
        "    def update_scores_by_tags(self, tag, resp):\n",
        "        updated_resp = []\n",
        "        \n",
        "        if \"sensevec\" in tag:\n",
        "            weight = self.weights['sensevec']\n",
        "        elif \"embedded\" in tag: #because embedded_1, embedded_2, ...\n",
        "            weight = self.weights['embedded']\n",
        "        else:\n",
        "            weight = self.weights[tag]\n",
        "\n",
        "        for doc in resp:\n",
        "                doc['updated_score'] =  doc['score']*weight #update score with weight\n",
        "                updated_resp.append(doc)\n",
        "\n",
        "        return (tag, updated_resp)\n",
        "\n",
        "    def mergen_for_multiple_ids(self, sorted_updated_resp_tags, multiple_ids):\n",
        "        merged = []\n",
        "        if self.method==\"max\":\n",
        "            \n",
        "            max_docs = []\n",
        "            for trec_id in multiple_ids:\n",
        "                \n",
        "                docs_id = [doc for doc in sorted_updated_resp_tags if doc['trec_id']==trec_id] #get docs with same id\n",
        "                #find doc with max-update-score\n",
        "                \n",
        "                max_score = max([doc['updated_score'] for doc in sorted_updated_resp_tags if doc['trec_id']==trec_id])\n",
        "                \n",
        "                max_doc = [doc for doc in docs_id if doc['updated_score']==max_score][0]\n",
        "                \n",
        "                max_docs.append(max_doc)\n",
        "                \n",
        "            \n",
        "            #add results with unique trec_id\n",
        "            for doc in sorted_updated_resp_tags:\n",
        "                if doc['trec_id'] not in multiple_ids:\n",
        "                    merged.append(doc)\n",
        "            #add result with largest score from max_docs\n",
        "            for max_doc in max_docs:\n",
        "                merged.append(max_doc)\n",
        "            #here sorting by score, not update-score, because update-score is only used for merging\n",
        "            \n",
        "            merged = sorted(merged, key=lambda doc: doc['score'], reverse=True)\n",
        "\n",
        "        else: #self.method==\"mean\"\n",
        "            avg_docs = []\n",
        "            for trec_id in multiple_ids:\n",
        "                from statistics import mean\n",
        "                #get information, first doc index 0 and then update score:\n",
        "                avg_doc = [doc for doc in sorted_updated_resp_tags if doc['trec_id']==trec_id][0] #already sorted, index[0] means the largest score\n",
        "                avg_doc['updated_score'] = mean([doc['updated_score'] for doc in sorted_updated_resp_tags if doc['trec_id']==trec_id])\n",
        "                avg_docs.append(avg_doc)\n",
        "            \n",
        "            for doc in sorted_updated_resp_tags:\n",
        "                if doc['trec_id'] not in multiple_ids:\n",
        "                    merged.append(doc)\n",
        "            #add result with avg score from same_trec_ids_max_value\n",
        "            for max_doc in avg_docs:\n",
        "                merged.append(max_doc)\n",
        "            merged = sorted(merged, key=lambda doc: doc['score'], reverse=True)\n",
        "\n",
        "        return merged\n",
        "if __name__ == \"__main__\":\n",
        "  #INPUT\n",
        "  topic_id = 1\n",
        "  fake_resp = {\n",
        "      'original': {'results':[\n",
        "                              {'trec_id':1, 'score':5},\n",
        "                              {'trec_id':2,'score':4},\n",
        "                              {'trec_id':3, 'score':4}\n",
        "                              ]},\n",
        "      'preprocessing': {'results':[\n",
        "                              {'trec_id':1, 'score':1},\n",
        "                              {'trec_id':2, 'score':1},\n",
        "                              {'trec_id':5, 'score':1}\n",
        "                              ]},\n",
        "      'annotation': {'results':[\n",
        "                              {'trec_id':4, 'score':3},\n",
        "                              {'trec_id':2, 'score':2},\n",
        "                              {'trec_id':3, 'score':1}\n",
        "                              ]},\n",
        "      'syns': {'results':[\n",
        "                              {'trec_id':5, 'score':2},\n",
        "                              {'trec_id':2, 'score':2},\n",
        "                              {'trec_id':4, 'score':1}\n",
        "                              ]},\n",
        "      'sensevec_1': {'results':[\n",
        "                              {'trec_id':1, 'score':3},\n",
        "                              {'trec_id':2, 'score':5},\n",
        "                              {'trec_id':3, 'score':5}\n",
        "                              ]},\n",
        "      'sensevec_2': {'results':[\n",
        "                              {'trec_id':1, 'score':2},\n",
        "                              {'trec_id':6, 'score':1},\n",
        "                              {'trec_id':3, 'score':1}\n",
        "                              ]},\n",
        "      'embedded_1': {'results':[\n",
        "                              {'trec_id':7, 'score':2},\n",
        "                              {'trec_id':8, 'score':1},\n",
        "                              {'trec_id':9, 'score':1}\n",
        "                              ]},\n",
        "      'embedded_2': {'results':[\n",
        "                              {'trec_id':1, 'score':2},\n",
        "                              {'trec_id':6, 'score':3.5},\n",
        "                              {'trec_id':7, 'score':2}\n",
        "                              ]},\n",
        "      'embedded_3': {'results':[\n",
        "                              {'trec_id':4, 'score':1},\n",
        "                              {'trec_id':5, 'score':2},\n",
        "                              {'trec_id':6, 'score':3}\n",
        "                              ]}\n",
        "  }\n",
        "  weights = {\n",
        "      'original':2,\n",
        "      'annotation': 1.75,\n",
        "      'sensevec': 1.5,\n",
        "      'embedded': 1.5,\n",
        "      'syns': 1,\n",
        "      'preprocessing': 1\n",
        "  }\n",
        "  print(\"Ergebnisse mit Max-Method\")\n",
        "  print(Merge(topic_id,fake_resp, weights, method='max').merging_res())\n",
        "  print(\"Ergebnisse mit Mean-Method\")\n",
        "  print(Merge(topic_id,fake_resp, weights, method='mean').merging_res())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ergebnisse mit Max-Method\n",
            "{'results': [{'trec_id': 1, 'score': 5, 'updated_score': 10}, {'trec_id': 2, 'score': 4, 'updated_score': 8}, {'trec_id': 3, 'score': 4, 'updated_score': 8}, {'trec_id': 6, 'score': 3.5, 'updated_score': 5.25}, {'trec_id': 4, 'score': 3, 'updated_score': 5.25}, {'trec_id': 7, 'score': 2, 'updated_score': 3.0}, {'trec_id': 5, 'score': 2, 'updated_score': 3.0}, {'trec_id': 8, 'score': 1, 'updated_score': 1.5}, {'trec_id': 9, 'score': 1, 'updated_score': 1.5}]}\n",
            "Ergebnisse mit Mean-Method\n",
            "{'results': [{'trec_id': 1, 'score': 5, 'updated_score': 4.3}, {'trec_id': 2, 'score': 4, 'updated_score': 4.4}, {'trec_id': 3, 'score': 4, 'updated_score': 4.6875}, {'trec_id': 6, 'score': 3.5, 'updated_score': 3.75}, {'trec_id': 4, 'score': 3, 'updated_score': 2.5833333333333335}, {'trec_id': 7, 'score': 2, 'updated_score': 3.0}, {'trec_id': 5, 'score': 2, 'updated_score': 2.0}, {'trec_id': 8, 'score': 1, 'updated_score': 1.5}, {'trec_id': 9, 'score': 1, 'updated_score': 1.5}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W06Cmt_BGQ7k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}