{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2-medium were not used when initializing GPT2ForTextRepresentation: ['wte.weight', 'wpe.weight', 'h.0.ln_1.weight', 'h.0.ln_1.bias', 'h.0.attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_attn.bias', 'h.0.attn.c_proj.weight', 'h.0.attn.c_proj.bias', 'h.0.ln_2.weight', 'h.0.ln_2.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_proj.weight', 'h.0.mlp.c_proj.bias', 'h.1.ln_1.weight', 'h.1.ln_1.bias', 'h.1.attn.bias', 'h.1.attn.c_attn.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_proj.weight', 'h.1.attn.c_proj.bias', 'h.1.ln_2.weight', 'h.1.ln_2.bias', 'h.1.mlp.c_fc.weight', 'h.1.mlp.c_fc.bias', 'h.1.mlp.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.2.ln_1.bias', 'h.2.attn.bias', 'h.2.attn.c_attn.weight', 'h.2.attn.c_attn.bias', 'h.2.attn.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.2.ln_2.weight', 'h.2.ln_2.bias', 'h.2.mlp.c_fc.weight', 'h.2.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.3.ln_1.weight', 'h.3.ln_1.bias', 'h.3.attn.bias', 'h.3.attn.c_attn.weight', 'h.3.attn.c_attn.bias', 'h.3.attn.c_proj.weight', 'h.3.attn.c_proj.bias', 'h.3.ln_2.weight', 'h.3.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.3.mlp.c_proj.weight', 'h.3.mlp.c_proj.bias', 'h.4.ln_1.weight', 'h.4.ln_1.bias', 'h.4.attn.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_attn.bias', 'h.4.attn.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.4.ln_2.weight', 'h.4.ln_2.bias', 'h.4.mlp.c_fc.weight', 'h.4.mlp.c_fc.bias', 'h.4.mlp.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.5.ln_1.weight', 'h.5.ln_1.bias', 'h.5.attn.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_attn.bias', 'h.5.attn.c_proj.weight', 'h.5.attn.c_proj.bias', 'h.5.ln_2.weight', 'h.5.ln_2.bias', 'h.5.mlp.c_fc.weight', 'h.5.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.5.mlp.c_proj.bias', 'h.6.ln_1.weight', 'h.6.ln_1.bias', 'h.6.attn.bias', 'h.6.attn.c_attn.weight', 'h.6.attn.c_attn.bias', 'h.6.attn.c_proj.weight', 'h.6.attn.c_proj.bias', 'h.6.ln_2.weight', 'h.6.ln_2.bias', 'h.6.mlp.c_fc.weight', 'h.6.mlp.c_fc.bias', 'h.6.mlp.c_proj.weight', 'h.6.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.7.ln_1.bias', 'h.7.attn.bias', 'h.7.attn.c_attn.weight', 'h.7.attn.c_attn.bias', 'h.7.attn.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.7.ln_2.weight', 'h.7.ln_2.bias', 'h.7.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.7.mlp.c_proj.weight', 'h.7.mlp.c_proj.bias', 'h.8.ln_1.weight', 'h.8.ln_1.bias', 'h.8.attn.bias', 'h.8.attn.c_attn.weight', 'h.8.attn.c_attn.bias', 'h.8.attn.c_proj.weight', 'h.8.attn.c_proj.bias', 'h.8.ln_2.weight', 'h.8.ln_2.bias', 'h.8.mlp.c_fc.weight', 'h.8.mlp.c_fc.bias', 'h.8.mlp.c_proj.weight', 'h.8.mlp.c_proj.bias', 'h.9.ln_1.weight', 'h.9.ln_1.bias', 'h.9.attn.bias', 'h.9.attn.c_attn.weight', 'h.9.attn.c_attn.bias', 'h.9.attn.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.9.ln_2.weight', 'h.9.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.9.mlp.c_fc.bias', 'h.9.mlp.c_proj.weight', 'h.9.mlp.c_proj.bias', 'h.10.ln_1.weight', 'h.10.ln_1.bias', 'h.10.attn.bias', 'h.10.attn.c_attn.weight', 'h.10.attn.c_attn.bias', 'h.10.attn.c_proj.weight', 'h.10.attn.c_proj.bias', 'h.10.ln_2.weight', 'h.10.ln_2.bias', 'h.10.mlp.c_fc.weight', 'h.10.mlp.c_fc.bias', 'h.10.mlp.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.11.ln_1.bias', 'h.11.attn.bias', 'h.11.attn.c_attn.weight', 'h.11.attn.c_attn.bias', 'h.11.attn.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.11.ln_2.weight', 'h.11.ln_2.bias', 'h.11.mlp.c_fc.weight', 'h.11.mlp.c_fc.bias', 'h.11.mlp.c_proj.weight', 'h.11.mlp.c_proj.bias', 'h.12.ln_1.weight', 'h.12.ln_1.bias', 'h.12.attn.bias', 'h.12.attn.c_attn.weight', 'h.12.attn.c_attn.bias', 'h.12.attn.c_proj.weight', 'h.12.attn.c_proj.bias', 'h.12.ln_2.weight', 'h.12.ln_2.bias', 'h.12.mlp.c_fc.weight', 'h.12.mlp.c_fc.bias', 'h.12.mlp.c_proj.weight', 'h.12.mlp.c_proj.bias', 'h.13.ln_1.weight', 'h.13.ln_1.bias', 'h.13.attn.bias', 'h.13.attn.c_attn.weight', 'h.13.attn.c_attn.bias', 'h.13.attn.c_proj.weight', 'h.13.attn.c_proj.bias', 'h.13.ln_2.weight', 'h.13.ln_2.bias', 'h.13.mlp.c_fc.weight', 'h.13.mlp.c_fc.bias', 'h.13.mlp.c_proj.weight', 'h.13.mlp.c_proj.bias', 'h.14.ln_1.weight', 'h.14.ln_1.bias', 'h.14.attn.bias', 'h.14.attn.c_attn.weight', 'h.14.attn.c_attn.bias', 'h.14.attn.c_proj.weight', 'h.14.attn.c_proj.bias', 'h.14.ln_2.weight', 'h.14.ln_2.bias', 'h.14.mlp.c_fc.weight', 'h.14.mlp.c_fc.bias', 'h.14.mlp.c_proj.weight', 'h.14.mlp.c_proj.bias', 'h.15.ln_1.weight', 'h.15.ln_1.bias', 'h.15.attn.bias', 'h.15.attn.c_attn.weight', 'h.15.attn.c_attn.bias', 'h.15.attn.c_proj.weight', 'h.15.attn.c_proj.bias', 'h.15.ln_2.weight', 'h.15.ln_2.bias', 'h.15.mlp.c_fc.weight', 'h.15.mlp.c_fc.bias', 'h.15.mlp.c_proj.weight', 'h.15.mlp.c_proj.bias', 'h.16.ln_1.weight', 'h.16.ln_1.bias', 'h.16.attn.bias', 'h.16.attn.c_attn.weight', 'h.16.attn.c_attn.bias', 'h.16.attn.c_proj.weight', 'h.16.attn.c_proj.bias', 'h.16.ln_2.weight', 'h.16.ln_2.bias', 'h.16.mlp.c_fc.weight', 'h.16.mlp.c_fc.bias', 'h.16.mlp.c_proj.weight', 'h.16.mlp.c_proj.bias', 'h.17.ln_1.weight', 'h.17.ln_1.bias', 'h.17.attn.bias', 'h.17.attn.c_attn.weight', 'h.17.attn.c_attn.bias', 'h.17.attn.c_proj.weight', 'h.17.attn.c_proj.bias', 'h.17.ln_2.weight', 'h.17.ln_2.bias', 'h.17.mlp.c_fc.weight', 'h.17.mlp.c_fc.bias', 'h.17.mlp.c_proj.weight', 'h.17.mlp.c_proj.bias', 'h.18.ln_1.weight', 'h.18.ln_1.bias', 'h.18.attn.bias', 'h.18.attn.c_attn.weight', 'h.18.attn.c_attn.bias', 'h.18.attn.c_proj.weight', 'h.18.attn.c_proj.bias', 'h.18.ln_2.weight', 'h.18.ln_2.bias', 'h.18.mlp.c_fc.weight', 'h.18.mlp.c_fc.bias', 'h.18.mlp.c_proj.weight', 'h.18.mlp.c_proj.bias', 'h.19.ln_1.weight', 'h.19.ln_1.bias', 'h.19.attn.bias', 'h.19.attn.c_attn.weight', 'h.19.attn.c_attn.bias', 'h.19.attn.c_proj.weight', 'h.19.attn.c_proj.bias', 'h.19.ln_2.weight', 'h.19.ln_2.bias', 'h.19.mlp.c_fc.weight', 'h.19.mlp.c_fc.bias', 'h.19.mlp.c_proj.weight', 'h.19.mlp.c_proj.bias', 'h.20.ln_1.weight', 'h.20.ln_1.bias', 'h.20.attn.bias', 'h.20.attn.c_attn.weight', 'h.20.attn.c_attn.bias', 'h.20.attn.c_proj.weight', 'h.20.attn.c_proj.bias', 'h.20.ln_2.weight', 'h.20.ln_2.bias', 'h.20.mlp.c_fc.weight', 'h.20.mlp.c_fc.bias', 'h.20.mlp.c_proj.weight', 'h.20.mlp.c_proj.bias', 'h.21.ln_1.weight', 'h.21.ln_1.bias', 'h.21.attn.bias', 'h.21.attn.c_attn.weight', 'h.21.attn.c_attn.bias', 'h.21.attn.c_proj.weight', 'h.21.attn.c_proj.bias', 'h.21.ln_2.weight', 'h.21.ln_2.bias', 'h.21.mlp.c_fc.weight', 'h.21.mlp.c_fc.bias', 'h.21.mlp.c_proj.weight', 'h.21.mlp.c_proj.bias', 'h.22.ln_1.weight', 'h.22.ln_1.bias', 'h.22.attn.bias', 'h.22.attn.c_attn.weight', 'h.22.attn.c_attn.bias', 'h.22.attn.c_proj.weight', 'h.22.attn.c_proj.bias', 'h.22.ln_2.weight', 'h.22.ln_2.bias', 'h.22.mlp.c_fc.weight', 'h.22.mlp.c_fc.bias', 'h.22.mlp.c_proj.weight', 'h.22.mlp.c_proj.bias', 'h.23.ln_1.weight', 'h.23.ln_1.bias', 'h.23.attn.bias', 'h.23.attn.c_attn.weight', 'h.23.attn.c_attn.bias', 'h.23.attn.c_proj.weight', 'h.23.attn.c_proj.bias', 'h.23.ln_2.weight', 'h.23.ln_2.bias', 'h.23.mlp.c_fc.weight', 'h.23.mlp.c_fc.bias', 'h.23.mlp.c_proj.weight', 'h.23.mlp.c_proj.bias', 'ln_f.weight', 'ln_f.bias']\n",
      "- This IS expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['gpt2.wte.weight', 'gpt2.wpe.weight', 'gpt2.h.0.ln_1.weight', 'gpt2.h.0.ln_1.bias', 'gpt2.h.0.attn.bias', 'gpt2.h.0.attn.masked_bias', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.0.ln_2.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.h.1.ln_1.bias', 'gpt2.h.1.attn.bias', 'gpt2.h.1.attn.masked_bias', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.1.ln_2.weight', 'gpt2.h.1.ln_2.bias', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.2.ln_1.weight', 'gpt2.h.2.ln_1.bias', 'gpt2.h.2.attn.bias', 'gpt2.h.2.attn.masked_bias', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.2.ln_2.weight', 'gpt2.h.2.ln_2.bias', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.3.attn.bias', 'gpt2.h.3.attn.masked_bias', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.3.ln_2.weight', 'gpt2.h.3.ln_2.bias', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.4.ln_1.weight', 'gpt2.h.4.ln_1.bias', 'gpt2.h.4.attn.bias', 'gpt2.h.4.attn.masked_bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.4.ln_2.bias', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.h.5.attn.bias', 'gpt2.h.5.attn.masked_bias', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.5.ln_2.weight', 'gpt2.h.5.ln_2.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.6.ln_1.weight', 'gpt2.h.6.ln_1.bias', 'gpt2.h.6.attn.bias', 'gpt2.h.6.attn.masked_bias', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.6.ln_2.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.7.ln_1.weight', 'gpt2.h.7.ln_1.bias', 'gpt2.h.7.attn.bias', 'gpt2.h.7.attn.masked_bias', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.7.ln_2.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.8.ln_1.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.8.attn.bias', 'gpt2.h.8.attn.masked_bias', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.8.ln_2.weight', 'gpt2.h.8.ln_2.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.9.ln_1.weight', 'gpt2.h.9.ln_1.bias', 'gpt2.h.9.attn.bias', 'gpt2.h.9.attn.masked_bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.9.ln_2.weight', 'gpt2.h.9.ln_2.bias', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.10.ln_1.weight', 'gpt2.h.10.ln_1.bias', 'gpt2.h.10.attn.bias', 'gpt2.h.10.attn.masked_bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.10.ln_2.weight', 'gpt2.h.10.ln_2.bias', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.11.ln_1.weight', 'gpt2.h.11.ln_1.bias', 'gpt2.h.11.attn.bias', 'gpt2.h.11.attn.masked_bias', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.h.11.ln_2.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.12.ln_1.weight', 'gpt2.h.12.ln_1.bias', 'gpt2.h.12.attn.bias', 'gpt2.h.12.attn.masked_bias', 'gpt2.h.12.attn.c_attn.weight', 'gpt2.h.12.attn.c_attn.bias', 'gpt2.h.12.attn.c_proj.weight', 'gpt2.h.12.attn.c_proj.bias', 'gpt2.h.12.ln_2.weight', 'gpt2.h.12.ln_2.bias', 'gpt2.h.12.mlp.c_fc.weight', 'gpt2.h.12.mlp.c_fc.bias', 'gpt2.h.12.mlp.c_proj.weight', 'gpt2.h.12.mlp.c_proj.bias', 'gpt2.h.13.ln_1.weight', 'gpt2.h.13.ln_1.bias', 'gpt2.h.13.attn.bias', 'gpt2.h.13.attn.masked_bias', 'gpt2.h.13.attn.c_attn.weight', 'gpt2.h.13.attn.c_attn.bias', 'gpt2.h.13.attn.c_proj.weight', 'gpt2.h.13.attn.c_proj.bias', 'gpt2.h.13.ln_2.weight', 'gpt2.h.13.ln_2.bias', 'gpt2.h.13.mlp.c_fc.weight', 'gpt2.h.13.mlp.c_fc.bias', 'gpt2.h.13.mlp.c_proj.weight', 'gpt2.h.13.mlp.c_proj.bias', 'gpt2.h.14.ln_1.weight', 'gpt2.h.14.ln_1.bias', 'gpt2.h.14.attn.bias', 'gpt2.h.14.attn.masked_bias', 'gpt2.h.14.attn.c_attn.weight', 'gpt2.h.14.attn.c_attn.bias', 'gpt2.h.14.attn.c_proj.weight', 'gpt2.h.14.attn.c_proj.bias', 'gpt2.h.14.ln_2.weight', 'gpt2.h.14.ln_2.bias', 'gpt2.h.14.mlp.c_fc.weight', 'gpt2.h.14.mlp.c_fc.bias', 'gpt2.h.14.mlp.c_proj.weight', 'gpt2.h.14.mlp.c_proj.bias', 'gpt2.h.15.ln_1.weight', 'gpt2.h.15.ln_1.bias', 'gpt2.h.15.attn.bias', 'gpt2.h.15.attn.masked_bias', 'gpt2.h.15.attn.c_attn.weight', 'gpt2.h.15.attn.c_attn.bias', 'gpt2.h.15.attn.c_proj.weight', 'gpt2.h.15.attn.c_proj.bias', 'gpt2.h.15.ln_2.weight', 'gpt2.h.15.ln_2.bias', 'gpt2.h.15.mlp.c_fc.weight', 'gpt2.h.15.mlp.c_fc.bias', 'gpt2.h.15.mlp.c_proj.weight', 'gpt2.h.15.mlp.c_proj.bias', 'gpt2.h.16.ln_1.weight', 'gpt2.h.16.ln_1.bias', 'gpt2.h.16.attn.bias', 'gpt2.h.16.attn.masked_bias', 'gpt2.h.16.attn.c_attn.weight', 'gpt2.h.16.attn.c_attn.bias', 'gpt2.h.16.attn.c_proj.weight', 'gpt2.h.16.attn.c_proj.bias', 'gpt2.h.16.ln_2.weight', 'gpt2.h.16.ln_2.bias', 'gpt2.h.16.mlp.c_fc.weight', 'gpt2.h.16.mlp.c_fc.bias', 'gpt2.h.16.mlp.c_proj.weight', 'gpt2.h.16.mlp.c_proj.bias', 'gpt2.h.17.ln_1.weight', 'gpt2.h.17.ln_1.bias', 'gpt2.h.17.attn.bias', 'gpt2.h.17.attn.masked_bias', 'gpt2.h.17.attn.c_attn.weight', 'gpt2.h.17.attn.c_attn.bias', 'gpt2.h.17.attn.c_proj.weight', 'gpt2.h.17.attn.c_proj.bias', 'gpt2.h.17.ln_2.weight', 'gpt2.h.17.ln_2.bias', 'gpt2.h.17.mlp.c_fc.weight', 'gpt2.h.17.mlp.c_fc.bias', 'gpt2.h.17.mlp.c_proj.weight', 'gpt2.h.17.mlp.c_proj.bias', 'gpt2.h.18.ln_1.weight', 'gpt2.h.18.ln_1.bias', 'gpt2.h.18.attn.bias', 'gpt2.h.18.attn.masked_bias', 'gpt2.h.18.attn.c_attn.weight', 'gpt2.h.18.attn.c_attn.bias', 'gpt2.h.18.attn.c_proj.weight', 'gpt2.h.18.attn.c_proj.bias', 'gpt2.h.18.ln_2.weight', 'gpt2.h.18.ln_2.bias', 'gpt2.h.18.mlp.c_fc.weight', 'gpt2.h.18.mlp.c_fc.bias', 'gpt2.h.18.mlp.c_proj.weight', 'gpt2.h.18.mlp.c_proj.bias', 'gpt2.h.19.ln_1.weight', 'gpt2.h.19.ln_1.bias', 'gpt2.h.19.attn.bias', 'gpt2.h.19.attn.masked_bias', 'gpt2.h.19.attn.c_attn.weight', 'gpt2.h.19.attn.c_attn.bias', 'gpt2.h.19.attn.c_proj.weight', 'gpt2.h.19.attn.c_proj.bias', 'gpt2.h.19.ln_2.weight', 'gpt2.h.19.ln_2.bias', 'gpt2.h.19.mlp.c_fc.weight', 'gpt2.h.19.mlp.c_fc.bias', 'gpt2.h.19.mlp.c_proj.weight', 'gpt2.h.19.mlp.c_proj.bias', 'gpt2.h.20.ln_1.weight', 'gpt2.h.20.ln_1.bias', 'gpt2.h.20.attn.bias', 'gpt2.h.20.attn.masked_bias', 'gpt2.h.20.attn.c_attn.weight', 'gpt2.h.20.attn.c_attn.bias', 'gpt2.h.20.attn.c_proj.weight', 'gpt2.h.20.attn.c_proj.bias', 'gpt2.h.20.ln_2.weight', 'gpt2.h.20.ln_2.bias', 'gpt2.h.20.mlp.c_fc.weight', 'gpt2.h.20.mlp.c_fc.bias', 'gpt2.h.20.mlp.c_proj.weight', 'gpt2.h.20.mlp.c_proj.bias', 'gpt2.h.21.ln_1.weight', 'gpt2.h.21.ln_1.bias', 'gpt2.h.21.attn.bias', 'gpt2.h.21.attn.masked_bias', 'gpt2.h.21.attn.c_attn.weight', 'gpt2.h.21.attn.c_attn.bias', 'gpt2.h.21.attn.c_proj.weight', 'gpt2.h.21.attn.c_proj.bias', 'gpt2.h.21.ln_2.weight', 'gpt2.h.21.ln_2.bias', 'gpt2.h.21.mlp.c_fc.weight', 'gpt2.h.21.mlp.c_fc.bias', 'gpt2.h.21.mlp.c_proj.weight', 'gpt2.h.21.mlp.c_proj.bias', 'gpt2.h.22.ln_1.weight', 'gpt2.h.22.ln_1.bias', 'gpt2.h.22.attn.bias', 'gpt2.h.22.attn.masked_bias', 'gpt2.h.22.attn.c_attn.weight', 'gpt2.h.22.attn.c_attn.bias', 'gpt2.h.22.attn.c_proj.weight', 'gpt2.h.22.attn.c_proj.bias', 'gpt2.h.22.ln_2.weight', 'gpt2.h.22.ln_2.bias', 'gpt2.h.22.mlp.c_fc.weight', 'gpt2.h.22.mlp.c_fc.bias', 'gpt2.h.22.mlp.c_proj.weight', 'gpt2.h.22.mlp.c_proj.bias', 'gpt2.h.23.ln_1.weight', 'gpt2.h.23.ln_1.bias', 'gpt2.h.23.attn.bias', 'gpt2.h.23.attn.masked_bias', 'gpt2.h.23.attn.c_attn.weight', 'gpt2.h.23.attn.c_attn.bias', 'gpt2.h.23.attn.c_proj.weight', 'gpt2.h.23.attn.c_proj.bias', 'gpt2.h.23.ln_2.weight', 'gpt2.h.23.ln_2.bias', 'gpt2.h.23.mlp.c_fc.weight', 'gpt2.h.23.mlp.c_fc.bias', 'gpt2.h.23.mlp.c_proj.weight', 'gpt2.h.23.mlp.c_proj.bias', 'gpt2.ln_f.weight', 'gpt2.ln_f.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "with open('data/generated_texts.txt') as f:\n",
    "    lines = f.read()\n",
    "generated_data=[]\n",
    "for line in lines.split(\"=\"*40 + \"\\n\"):\n",
    "    if len(line.replace(\"\\n\",\"\"))!=0:\n",
    "        generated_data.append(line.replace(\"\\n\",\"\"))\n",
    "generated_data={(i+1): generated_data[i] for i in range(0,len(generated_data))}\n",
    "\n",
    "\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "gpt2_model = RepresentationModel(\n",
    "        model_type=\"gpt2\",\n",
    "        model_name=\"gpt2-medium\",\n",
    "        use_cuda=False\n",
    "    )\n",
    "generated_vectors = gpt2_model.encode_sentences(list(generated_data.values()), combine_strategy='mean')\n",
    "\n",
    "from scipy.spatial import distance\n",
    "def similarity(doc, topicId):\n",
    "    doc_vector = gpt2_model.encode_sentences([doc], combine_strategy='mean')\n",
    "    similarity_score = 1- distance.cosine(doc_vector,generated_vectors[topicId-1])\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The first thing you need to do is to find a comfortable bed.It's not always easy to find a comfortable bed, but it's always possible.You can find a bed in a small room, in a room with a window, or in a room with a window.You can find a bed in a room with a window, in a room with a window. You can find a bed in a room with a window, in a room with a\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(file):\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    buffer = []\n",
    "    i=1\n",
    "    for title in root.iter('title'):\n",
    "        if i<=int(n_topics):\n",
    "            buffer.append(title.text.strip())\n",
    "        i=i+1\n",
    "    return buffer\n",
    "def remove_htmlTags(doc):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', doc)\n",
    "def get_texts_title_snippet(trecIds_titles_snippets):\n",
    "    docs={}\n",
    "    for trec_id, title, snippet in trecIds_titles_snippets:\n",
    "        docs[trec_id] = title + \". \" + snippet\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def update_similarity_pagerank(similarity_scores,resp, statusAdd):\n",
    "    '''\n",
    "    spam rank high -> more spam\n",
    "    '''\n",
    "    new_resp = []\n",
    "    if statusAdd==False: #using directly similarity score for reranking\n",
    "        for doc in resp['results']:\n",
    "            trec_id = doc['trec_id']\n",
    "            spam_rank=float()\n",
    "            \n",
    "            if str(doc['spam_rank'])==\"null\":\n",
    "                spam_rank = statistics.mean([doc['spam_rank'] for doc in resp['results'] if str(doc['spam_rank'])!=\"null\" ])\n",
    "            else:\n",
    "                spam_rank=float(doc['spam_rank'])\n",
    "            similarity_score= similarity_scores[trec_id]\n",
    "            \n",
    "            #change end_score\n",
    "            doc['score'] = similarity_score/spam_rank\n",
    "            new_resp.append(doc)\n",
    "    else:\n",
    "        for doc in resp['results']:\n",
    "            trec_id = doc['trec_id']\n",
    "            \n",
    "            spam_rank=float()\n",
    "            if str(doc['spam_rank'])==\"null\":\n",
    "                spam_rank = statistics.mean([doc['spam_rank'] for doc in resp['results'] if str(doc['spam_rank'])!=\"null\" ])\n",
    "            else:\n",
    "                spam_rank=float(doc['spam_rank'])\n",
    "            \n",
    "            relevance_score=float(doc['score'])\n",
    "            similarity_score= similarity_scores[trec_id]\n",
    "            \n",
    "            #change end_score\n",
    "            doc['score'] = relevance_score*(1+similarity_score) - spam_rank\n",
    "            new_resp.append(doc)\n",
    "            \n",
    "    return {'results':sorted(new_resp,key= lambda doc: doc['score'], reverse=True)}\n",
    "def update_response(similarity_scores,resp, statusAdd):\n",
    "    new_resp = []\n",
    "    if statusAdd==False: #using directly similarity score for reranking\n",
    "        for doc in resp['results']:\n",
    "            trec_id = doc['trec_id']\n",
    "            #relevance_score=float(doc['score'])\n",
    "            similarity_score= similarity_scores[trec_id]\n",
    "            \n",
    "            #change end_score\n",
    "            doc['score'] = similarity_score\n",
    "            new_resp.append(doc)\n",
    "    else:\n",
    "        for doc in resp['results']:\n",
    "            trec_id = doc['trec_id']\n",
    "            relevance_score=float(doc['score'])\n",
    "            similarity_score= similarity_scores[trec_id]\n",
    "            \n",
    "            #change end_score\n",
    "            doc['score'] = relevance_score*(1+similarity_score)\n",
    "            new_resp.append(doc)\n",
    "            \n",
    "    return {'results':sorted(new_resp,key= lambda doc: doc['score'], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatnoir_api(topicID, topic, size, spamRank, statusAdd):\n",
    "    \n",
    "    url = 'https://www.chatnoir.eu/api/v1/_search'\n",
    "    request_data = {\n",
    "        \"apikey\": \"67fac2d9-0f98-4c19-aab0-18c848bfa130\",\n",
    "        \"query\": topic,\n",
    "        \"size\": size,\n",
    "        \"index\": [\"cw12\"],\n",
    "    }\n",
    "    def chatnoir_req():\n",
    "        try:\n",
    "            response_=requests.post(url, data=request_data)\n",
    "            response_.raise_for_status()\n",
    "            return response_.json()\n",
    "        except requests.exceptions.HTTPError:\n",
    "            time.sleep(1)\n",
    "            return chatnoir_req()    \n",
    "    resp = chatnoir_req()\n",
    "    print(\"=\"*40 + \"GET RESPONSES\" + \"=\"*40)\n",
    "    results = resp['results']\n",
    "    trecIds_titles_snippets = [(results[i]['trec_id'],remove_htmlTags(results[i]['title']), remove_htmlTags(results[i]['snippet'])) for i in range(0,len(results))]\n",
    "    docs= get_texts_title_snippet(trecIds_titles_snippets) #only titles and snippets\n",
    "    \n",
    "    similarity_scores={}\n",
    "    for trec_id, doc in docs.items():\n",
    "        print(remove_htmlTags(doc))\n",
    "        score = similarity(doc, topicID)\n",
    "        print(score)\n",
    "        similarity_scores[trec_id]=score\n",
    "    #print(similarity_scores)\n",
    "    \n",
    "    #similarity_scores = dict(sorted(x.items(), key=lambda item: item[1]))\n",
    "    if spamRank==False:\n",
    "        resp=update_response(similarity_scores,resp, statusAdd)\n",
    "    else:\n",
    "        resp=update_similarity_pagerank(similarity_scores,resp, statusAdd)\n",
    "    #print(resp)\n",
    "    return similarity_scores, resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(spamRank, statusAdd):\n",
    "    topics = get_titles(file)\n",
    "    print(topics[0])\n",
    "    out = open(\"data/output_similarity_\"+str(spamRank)+\"_\"+str(statusAdd)+\".run\", \"w\")\n",
    "    answers = []\n",
    "    \n",
    "    index=1 #TOPIC ID\n",
    "    for topic in topics:\n",
    "        print(\"Getting response for\", topic)\n",
    "        answers.append(chatnoir_api(index, topic, size, spamRank, statusAdd)[1])\n",
    "        #if(index==1):\n",
    "        #    break\n",
    "        index=index+1\n",
    "        \n",
    "    topicId = 1\n",
    "    for topic in answers:\n",
    "        print(topic)\n",
    "        rank = 1\n",
    "        for response in topic['results']:\n",
    "            buffer = topicId, \"Q0\", response['trec_id'], rank, response['score'], \"JackSparrowVanilla\"\n",
    "            out.write(\" \".join(map(str, buffer)) + \"\\n\")\n",
    "            rank += 1\n",
    "        topicId += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics=50\n",
    "file=\"data/topics-task-2.xml\"\n",
    "statusAdd=False\n",
    "spamRank=True#using only similarity for reranking\n",
    "size=15\n",
    "main(spamRank, statusAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run compare_with_baseline.py data/touche2020-task2-relevance-withbaseline.qrel data/base_chatnoir.run data/output_similarity_False_True.run 'ndcg_cut_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics=50\n",
    "file=\"data/topics-task-2.xml\"\n",
    "statusAdd=True\n",
    "spamRank=True#using similarity score to update relevance score\n",
    "size=15\n",
    "main(spamRank, statusAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run compare_with_baseline.py data/touche2020-task2-relevance-withbaseline.qrel data/base_chatnoir.run data/output_similarity_True_True.run 'ndcg_cut_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_topic(index, size, spamRank, statusAdd):\n",
    "    topics = get_titles(file)\n",
    "    specific_topic=topics[index-1]\n",
    "    print(specific_topic)\n",
    "    print(\"GENERATED TEXT: \" + generated_data[index])\n",
    "    similarity_scores, resp = chatnoir_api(index, specific_topic, size, spamRank, statusAdd)\n",
    "    print(similarity_scores)\n",
    "    return similarity_scores, resp\n",
    "n_topics=50\n",
    "file=\"data/topics-task-2.xml\"\n",
    "\n",
    "similarity_scores, resp = specific_topic(36, 15,True, True)\n",
    "statistics.mean(similarity_scores.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (internetsuchmaschine)",
   "language": "python",
   "name": "internetsuchmaschine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
