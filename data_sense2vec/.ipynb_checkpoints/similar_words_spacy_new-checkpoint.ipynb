{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sense2vec import Sense2VecComponent\n",
    "s2v = nlp.add_pipe(\"sense2vec\")\n",
    "s2v.from_disk(\"s2v_reddit_2015_md/s2v_old\")\n",
    "\n",
    "from sense2vec import Sense2Vec\n",
    "standalone_s2v = Sense2Vec().from_disk(\"./s2v_reddit_2015_md/s2v_old\")\n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "n_topics=50\n",
    "def get_titles(file):\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    buffer = []\n",
    "    i=1\n",
    "    for title in root.iter('title'):\n",
    "        if i<=int(n_topics):\n",
    "            buffer.append(title.text.strip())\n",
    "        i=i+1\n",
    "    return buffer\n",
    "file=\"topics-task-2.xml\"\n",
    "topics=get_titles(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(word, topn):\n",
    "    ms = nlp.vocab.vectors.most_similar(np.asarray([nlp.vocab.vectors[nlp.vocab.strings[word]]]), n=topn)\n",
    "    words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    return words\n",
    "\n",
    "def similar_words_with_wordembeddings(topic, topn):\n",
    "    tags = ['CD',\n",
    "            'JJ',\n",
    "            'RB', \n",
    "            'NN', 'NNS','NNP','NNPS', \n",
    "            'VB']\n",
    "    pos_tags = ['PROPN','VERB','NOUN','NUM']\n",
    "\n",
    "    doc = nlp(topic)\n",
    "    similar_words={}\n",
    "    for token in doc:\n",
    "        top_similar_words=[]\n",
    "        if token.tag_ in tags or token.pos_ in pos_tags:\n",
    "            if token.lemma_ not in STOP_WORDS or token.text not in STOP_WORDS:\n",
    "                print(token.text)\n",
    "                try:\n",
    "                    similar_words_by_wordembeddings = get_similar_words(token.text, topn)\n",
    "                    \n",
    "                    for word in similar_words_by_wordembeddings:\n",
    "                        if (word != token.text) and (nlp(word)[0].lemma_ != token.lemma_):\n",
    "                            top_similar_words.append(nlp(word)[0].lemma_.lower())\n",
    "                except ValueError as err:\n",
    "                    print(err)\n",
    "                print(set(top_similar_words))\n",
    "                similar_words[token.text]=list(set(top_similar_words))\n",
    "                print(\"=\"*80)\n",
    "    return similar_words\n",
    "    \n",
    "def get_similars_sense2vec(topic, topn):\n",
    "    tags = ['CD',\n",
    "            'JJ',\n",
    "            'RB', \n",
    "            'NN', 'NNS','NNP','NNPS', \n",
    "            'VB']\n",
    "    pos_tags = ['PROPN','VERB','NOUN','NUM']\n",
    "\n",
    "    doc = nlp(topic)\n",
    "    similar_words={}\n",
    "    for token in doc:\n",
    "        top_similar_words=[]\n",
    "        if token.tag_ in tags or token.pos_ in pos_tags:\n",
    "            if token.lemma_ not in STOP_WORDS or token.text not in STOP_WORDS:\n",
    "                try:\n",
    "                    for e in token._.s2v_most_similar(topn):\n",
    "                        word = e[0][0].strip() #get only word from ((word, tag), proba)\n",
    "                        if (word != token.text) and (nlp(word)[0].lemma_ != token.lemma_):\n",
    "                            top_similar_words.append(word)\n",
    "                except ValueError as err:\n",
    "                    for ent in doc.ents:\n",
    "                        if ent.text == token.text:\n",
    "                            try:\n",
    "                                for e in ent._.s2v_most_similar(topn):\n",
    "                                    word = e[0][0].strip() #get only word from ((word, tag), proba)\n",
    "                                    if (word != token.text) and (nlp(word)[0].lemma_ != token.lemma_):\n",
    "                                        top_similar_words.append(word)\n",
    "\n",
    "                            except ValueError as err:\n",
    "                                print(token._.s2v_other_senses == ent._.s2v_other_senses)\n",
    "                                query = token._.s2v_other_senses[0] #get first similar words by entity_tag\n",
    "                                print(token._.s2v_other_senses[0])\n",
    "                                print(ent._.s2v_other_senses[0])\n",
    "                                for e in standalone_s2v.most_similar(query, n=topn):\n",
    "                                    word = e[0].split(\"|\")[0].strip() #get only word from (word|tag, proba)\n",
    "                                    if (word != token.text) and (nlp(word)[0].lemma_ != token.lemma_):\n",
    "                                        top_similar_words.append(word)\n",
    "                print(set(top_similar_words))\n",
    "                similar_words[token.text]=list(set(top_similar_words))\n",
    "                print(\"=\"*80)\n",
    "    return similar_words\n",
    "\n",
    "def similarwords_replace(query, similar_words):\n",
    "    import copy\n",
    "    \n",
    "    expanded_queries=[]\n",
    "    new=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for word, similars in similar_words.items():\n",
    "        for similar_word in similars:\n",
    "            print(similar_word)\n",
    "            new_query=copy.deepcopy(query).replace(word, similar_word)\n",
    "            expanded_queries.append(new_query)\n",
    "    \n",
    "    keys = similar_words.keys()\n",
    "    for key in keys:\n",
    "        for other_key in keys:\n",
    "            if other_key!=key:\n",
    "                for sw in similar_words[key]:\n",
    "                    pos_key = [ i for i in range(len(query.split(\" \"))) if query.split(\" \")[i]==key]\n",
    "                    #print(pos_key)\n",
    "                    n = copy.deepcopy(query).replace(key, sw)\n",
    "                    pos = [i+len(sw.split(\" \"))-1 for i in pos_key]\n",
    "                    #print(pos)\n",
    "                    #laptop 1 - desktop PC 1,2\n",
    "                    for p in pos_key:\n",
    "                        pos.append(p)\n",
    "                    pos = list(set(pos))\n",
    "                    print(pos)\n",
    "                    new.append(n)\n",
    "                    for swo in similar_words[other_key]:\n",
    "                        pos_other = [ i for i in range(len(n.split(\" \"))) if n.split(\" \")[i]==other_key and i not in pos]\n",
    "                        print(pos_other)\n",
    "                        print(\"========\")\n",
    "                        import numpy\n",
    "                        text_as_arr = numpy.array(n.split(\" \"))\n",
    "                        for idx in pos_other:\n",
    "                            text_as_arr[idx]=swo\n",
    "                        #nn = copy.deepcopy(n).replace(other_key, swo)\n",
    "                        nn = \" \".join(text_as_arr)\n",
    "                        new.append(nn)\n",
    "                    print(\"=======================================================\")\n",
    "    return new, expanded_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = get_similars_sense2vec(topics[0], topn=2)\n",
    "print(r)\n",
    "a,b=similarwords_replace(topics[0], r)\n",
    "print(topics[0])\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = similar_words_with_wordembeddings(topics[0], 2)\n",
    "print(rr)\n",
    "a,b=similarwords_replace(topics[0], rr)\n",
    "print(topics[0])\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-session",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senttovec",
   "language": "python",
   "name": "senttovec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
