{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassQueryExpansion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l9v038h4X2o",
        "outputId": "27d9d845-a9c6-4a6f-dcb8-2fd5d6dd0e49"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.7.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiM3uE574jJE",
        "outputId": "c3665926-5c82-4573-f12f-97f36d2c95a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofd1_jyo66qm",
        "outputId": "8837dff1-0b2b-46a6-9b81-1673620e7d2f"
      },
      "source": [
        "!unzip -u \"/content/drive/MyDrive/Colab_Notebooks/s2v_reddit_2015_md.zip\" -d \"/content\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab_Notebooks/s2v_reddit_2015_md.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZeSKmYP4ZVH",
        "outputId": "e3ece9eb-9632-4fb1-c291-9505392af273"
      },
      "source": [
        "!pip install sense2vec"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sense2vec in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (3.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (1.19.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sense2vec) (3.7.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (0.8.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.7.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.3.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (8.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (20.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (54.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.7.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->sense2vec) (3.4.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.0.4)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.0->sense2vec) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->sense2vec) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU3aF8xk4fCd",
        "outputId": "10698c07-a194-4103-c0d1-7cceb5bfd9d8"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-30 12:14:23.127663: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Requirement already satisfied: en-core-web-md==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl#egg=en_core_web_md==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (54.1.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.7)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M88ATkqs9LKh",
        "outputId": "723858d5-7392-4abe-aa89-ddfc75945d4f"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVmheE4U9Yo3",
        "outputId": "0265c69d-c525-4fd4-ed09-e47453d10b63"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import  wordnet\n",
        "from typing import List\n",
        "import spacy\n",
        "#en_core_web_md\n",
        "import string\n",
        "from nltk.corpus import wordnet\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "#packages to find similar words by wordembedding and sense2vec\n",
        "from sense2vec import Sense2VecComponent \n",
        "from sense2vec import Sense2Vec #for standalone"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlTrnvPV4tnd",
        "outputId": "8af17070-2ea4-4e4f-abc3-eabc9d088f0d"
      },
      "source": [
        "#!/usr/bin/python\n",
        "\n",
        "\n",
        "#global tags\n",
        "\n",
        "class QueryExpansion:\n",
        "\n",
        "    def __init__(self, query: List[str], top_syns: int = 5, top_similar: int=2):\n",
        "        self.original_query = query\n",
        "        self.nlp = spacy.load(\"en_core_web_md\")\n",
        "        self.top_syns = top_syns\n",
        "\n",
        "        self.sense = self.nlp.add_pipe(\"sense2vec\").from_disk(\"s2v_reddit_2015_md/s2v_old\")\n",
        "        self.standalone_sense = Sense2Vec().from_disk(\"./s2v_reddit_2015_md/s2v_old\") #it is not dubplicated\n",
        "        \n",
        "        self.tags = ['CD','JJ','RB', 'NN', 'NNS','NNP','NNPS', 'VB']\n",
        "        self.pos_tags = ['PROPN','VERB','NOUN','NUM']\n",
        "\n",
        "        self.top_similar = top_similar\n",
        "\n",
        "    # Query Expansion\n",
        "    def expansion(self, relation: bool = False, synonyms: bool = False, sensevec: bool=False, embedded: bool=False):\n",
        "        result = []\n",
        "        \n",
        "        if relation:\n",
        "            result = [*result, *self.get_comparation_superlation_nouns_from_original_data()]\n",
        "\n",
        "        if synonyms:\n",
        "            result = [*result, *self.synonyms()]\n",
        "        \n",
        "        if sensevec:\n",
        "            result = [*result, *self.similarwords_sensevec()] #each topic there are multi expanded queries by similar words because of replacing methods\n",
        "\n",
        "        if embedded:\n",
        "            result = [*result, *self.similarwords_wordembedding()]\n",
        "\n",
        "        # ToDo combine original or return only new ones\n",
        "        return result\n",
        "\n",
        "    def similarwords_replace(self, query, similar_words):\n",
        "        import copy\n",
        "        expanded_queries=[]\n",
        "        for word, similar_words in similar_words.items():\n",
        "            for similar_word in similar_words:\n",
        "                new_query=copy.deepcopy(query).replace(word, similar_word)\n",
        "                expanded_queries.append(new_query)\n",
        "        return expanded_queries\n",
        "\n",
        "    def similarwords_wordembedding(self):\n",
        "        result = []\n",
        "        for query in self.original_query:\n",
        "            doc = self.nlp(query)\n",
        "            #expanded queries\n",
        "            expanded_queries=[]\n",
        "            similar_words={}\n",
        "            for token in doc:\n",
        "                top_similar_words=[]\n",
        "                if token.tag_ in self.tags or token.pos_ in self.pos_tags:\n",
        "                    if token.lemma_ not in STOP_WORDS or token.text not in STOP_WORDS:\n",
        "                        try:\n",
        "                            word=token.lemma_\n",
        "                            #print(word)\n",
        "                            ms = self.nlp.vocab.vectors.most_similar(np.asarray([self.nlp.vocab.vectors[self.nlp.vocab.strings[word]]]), n=self.top_similar)\n",
        "                            #print(ms)\n",
        "                            similar_embedding = [self.nlp.vocab.strings[w] for w in ms[0][0]] #get only text from most_similar\n",
        "                            \n",
        "                            #checking again if similar words are the same word\n",
        "                            for word in similar_embedding:\n",
        "                                if (word != token.text) and (self.nlp(word)[0].lemma_ != token.lemma_):\n",
        "                                    top_similar_words.append(self.nlp(word)[0].lemma_.lower())\n",
        "                        except ValueError as err:\n",
        "                            print(err)\n",
        "                        \n",
        "                        similar_words[token.text]=list(set(top_similar_words)) #only unique words\n",
        "            \n",
        "            #replace with similar words for new queries.\n",
        "            expanded_queries = self.similarwords_replace(query, similar_words)\n",
        "            result.append(expanded_queries)\n",
        "        return result\n",
        "        \n",
        "    def similarwords_sensevec(self):\n",
        "        result=[]\n",
        "        for query in self.original_query:\n",
        "            doc = self.nlp(query)\n",
        "            expanded_queries=[]\n",
        "            similar_words={}\n",
        "            for token in doc:\n",
        "                top_similar_words=[]\n",
        "                if token.tag_ in self.tags or token.pos_ in self.pos_tags:\n",
        "                    if token.lemma_ not in STOP_WORDS or token.text not in STOP_WORDS:\n",
        "                        try:\n",
        "                            for e in token._.s2v_most_similar(self.top_similar):\n",
        "                                word = e[0][0].strip() #get only word from ((word, tag), proba)\n",
        "                                if (word != token.text) and (self.nlp(word)[0].lemma_ != token.lemma_):\n",
        "                                    top_similar_words.append(word)\n",
        "                        except ValueError as err:\n",
        "                            for ent in doc.ents:\n",
        "                                if ent.text == token.text:\n",
        "                                    try:\n",
        "                                        for e in ent._.s2v_most_similar(self.top_similar):\n",
        "                                            word = e[0][0].strip() #get only word from ((word, tag), proba)\n",
        "                                            if (word != token.text) and (self.nlp(word)[0].lemma_ != token.lemma_):\n",
        "                                                top_similar_words.append(word)\n",
        "\n",
        "                                    except ValueError as err:\n",
        "                                        \n",
        "                                        query = token._.s2v_other_senses[0] #get first similar words by entity_tag\n",
        "                                        for e in self.standalone_s2v.most_similar(query, n=self.top_similar):\n",
        "                                            word = e[0].split(\"|\")[0].strip() #get only word from (word|tag, proba)\n",
        "                                            if (word != token.text) and (self.nlp(word)[0].lemma_ != token.lemma_):\n",
        "                                                top_similar_words.append(word)\n",
        "                        \n",
        "                        similar_words[token.text]=list(set(top_similar_words))\n",
        "            expanded_queries=self.similarwords_replace(query, similar_words)\n",
        "            result.append(expanded_queries)\n",
        "        return result\n",
        "\n",
        "    def remove_punc(self, query: str):\n",
        "        table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "        title = query.translate(table)\n",
        "        return str(title)\n",
        "\n",
        "    def synonyms(self):\n",
        "        result = []\n",
        "\n",
        "        for query in self.original_query:\n",
        "            new_title = self.remove_punc(query)\n",
        "            syn_pro_title = list()\n",
        "            temp = new_title\n",
        "            new_title = self.nlp(new_title)\n",
        "            for token in new_title:\n",
        "                syn_token = self.find_syns_word(token)\n",
        "                syn_pro_title.extend(\n",
        "                    [syn for syn in list(set(syn_token)) if\n",
        "                     syn != str(token.text)])  # distinct and remove the same words\n",
        "            # print(syn_pro_title)\n",
        "            # synonyms_by_titles.writelines(\" \".join(list(set(syn_pro_title))) + \"\\n\")\n",
        "            # ToDo temp(org) + syns or only syns?\n",
        "            result.append(temp + \" \" + \" \".join(list(set(syn_pro_title))))\n",
        "        return result\n",
        "\n",
        "    def find_syns_word(self, token: str):\n",
        "        syn_token = []\n",
        "        if (token.pos_ == \"NOUN\"):\n",
        "            # ToDo automate wordnet install\n",
        "            for synset in wordnet.synsets(token.lemma_):\n",
        "                for lemma in synset.lemmas()[:self.top_syns]:  # top 5 synonyms\n",
        "                    if \"_\" not in lemma.name():  # not include the words with _ ex: basketball_game\n",
        "                        syn_token.append(lemma.name())\n",
        "                    else:\n",
        "                        for w in lemma.name().split(\"_\"):\n",
        "                            if w not in STOP_WORDS:\n",
        "                                syn_token.append(\n",
        "                                    w)  # add words with _ to two words ex. laptop_computer -> laptop and computer\n",
        "        syn_token = [w for w in syn_token if w != \" \" and len(w) != 0]\n",
        "\n",
        "        # ToDo remove random influence\n",
        "        if len(syn_token) > 5:\n",
        "            return random.sample(syn_token,k=5)  # after top 5 synonyms + splited words -> long titles -> reducing the syns random with 5\n",
        "        else:\n",
        "            return syn_token\n",
        "\n",
        "    def get_comparation_superlation_nouns_from_original_data(self):\n",
        "        result =[]\n",
        "\n",
        "        for query in self.original_query:\n",
        "            nouns_as_string = []\n",
        "            doc = self.nlp(query)\n",
        "            annotations = ['CC', 'CD',\n",
        "                           'JJ', 'JJR', 'JJS',\n",
        "                           'RB', 'RBR', 'RBS',\n",
        "                           'NN', 'NNS', 'NNP', 'NNPS',\n",
        "                           'VB']\n",
        "            for token in doc:\n",
        "                if token.tag_ in annotations:\n",
        "                    nouns_as_string.append(token.text)\n",
        "            result.append(' '.join(nouns_as_string))\n",
        "        return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = [\"What is the difference between sex and love?\",\n",
        "             \"What is the difference between sex and love?\",\n",
        "             \"Which is better, a laptop or a desktop?\"]\n",
        "\n",
        "    print(\"Org. query:\")\n",
        "\n",
        "    for i in query:\n",
        "        print(i)\n",
        "\n",
        "    expansion = QueryExpansion(query)\n",
        "\n",
        "    print(\"None:\")\n",
        "    for i in expansion.expansion():\n",
        "        print(i)\n",
        "    \n",
        "    print(\"Relation:\")\n",
        "    for i in expansion.expansion(relation=True):\n",
        "        print(i)\n",
        "\n",
        "    print(\"Synonyms:\")\n",
        "    for i in expansion.expansion(synonyms=True):\n",
        "        print(i)\n",
        "\n",
        "    print(\"sensevec\")\n",
        "    for i in expansion.expansion(sensevec=True):\n",
        "        print(i)\n",
        "\n",
        "    print(\"embedded:\")\n",
        "    for i in expansion.expansion(embedded=True):\n",
        "        print(i)\n",
        "\n",
        "    print(\"Both:\")\n",
        "    for i in expansion.expansion(sensevec=True, embedded=True):\n",
        "        print(i)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Org. query:\n",
            "What is the difference between sex and love?\n",
            "What is the difference between sex and love?\n",
            "Which is better, a laptop or a desktop?\n",
            "None:\n",
            "Relation:\n",
            "difference sex and love\n",
            "difference sex and love\n",
            "better laptop or desktop\n",
            "Synonyms:\n",
            "What is the difference between sex and love dear dearest gender opinion\n",
            "What is the difference between sex and love deviation sexuality lovemaking passion departure sexual turn\n",
            "Which is better a laptop or a desktop background screen computer\n",
            "sensevec\n",
            "['What is the real difference between sex and love?', 'What is the actual difference between sex and love?', 'What is the difference between only sex and love?', 'What is the difference between actual sex and love?', 'What is the difference between sex and real love?']\n",
            "['What is the real difference between sex and love?', 'What is the actual difference between sex and love?', 'What is the difference between only sex and love?', 'What is the difference between actual sex and love?', 'What is the difference between sex and real love?']\n",
            "['Which is better, a desktop computer or a desktop?', 'Which is better, a desktop PC or a desktop?', 'Which is better, a laptop or a Windows desktop?']\n",
            "embedded:\n",
            "['What is the differance between sex and love?']\n",
            "['What is the differance between sex and love?']\n",
            "['Which is better, a portable or a desktop?', 'Which is better, a lap or a desktop?', 'Which is better, a laptop or a destop?']\n",
            "Both:\n",
            "['What is the real difference between sex and love?', 'What is the actual difference between sex and love?', 'What is the difference between only sex and love?', 'What is the difference between actual sex and love?', 'What is the difference between sex and real love?']\n",
            "['What is the real difference between sex and love?', 'What is the actual difference between sex and love?', 'What is the difference between only sex and love?', 'What is the difference between actual sex and love?', 'What is the difference between sex and real love?']\n",
            "['Which is better, a desktop computer or a desktop?', 'Which is better, a desktop PC or a desktop?', 'Which is better, a laptop or a Windows desktop?']\n",
            "['What is the differance between sex and love?']\n",
            "['What is the differance between sex and love?']\n",
            "['Which is better, a portable or a desktop?', 'Which is better, a lap or a desktop?', 'Which is better, a laptop or a destop?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-v0_yayF7BE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0611b99-3dd4-4d6e-eca6-93cc15d9f46d"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "word=\"difference\"\n",
        "top_similar=2\n",
        "ms = nlp.vocab.vectors.most_similar(np.asarray([nlp.vocab.vectors[nlp.vocab.strings[word]]]))\n",
        "similar_embedding = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
        "similar_embedding"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_core_web_sm' (2.2.5) specifies an under-constrained spaCy version requirement: >=2.2.2. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.5,<3.1.0\n",
            "  warnings.warn(warn_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['differance']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nl1OoySGmpC"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5t6pqOBHtQ3"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nygPkh_lHk4V"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}